---
title: "Deep Learning avec R : réseaux de neurones convolutionnels avec la librairie keras"
author: "Antoine"
date: "??/??/????"
output: 
  html_document:
    toc: true
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)

library(dplyr)

```

# Présentation de la librairie Keras

## Origine de la librairie

Keras est une API Python qui permet de créer et entraîner des modèles de Deep Learning. Elle a l'avantage d'offrir une interface simple et une syntaxe claire. Elle permet également d'accéder à de nombreux modèles pré-entraînés qui peuvent ensuite facilement être _fine-tunés_ sur d'autres données. Elle a été récemment intégrée à `TensorFlow` mais peut s'appuyer sur d'autres backends.

Keras dispose aussi d'une [documentation très complète](https://keras.io/api/), et sa popularité permet de retrouver de nombreux exemples d'utilisation en ligne. Elle est le plus souvent utilisée grâce au package Python `keras` ou via TensorFlow avec `tf.keras`, qui permettent d'exploiter facilement ses différentes méthodes.   

## Installation du package sur R  

Grâce notamment à la puissance de `reticulate`, un package permettant de faire tourner Python depuis une session R, on dispose maintenant d'un package R `keras`, que nous vous présentons ici. Nous utiliserons la dernière version disponible, le package `keras3`. La première étape est bien sur d'installer le package avec `install.packages("keras3")` puis de lancer la fonction `keras3::install_keras()` qui permet d'installer Python et un environnement anaconda dédié.

```{r eval=FALSE}
library(keras3)
install_keras()
```

Si vous avez du GPU, vous pouvez vérifier que votre devise a bien été repérée avec l'instruction suivante (normalement, `tensorflow` a été installé au moment de l'installation de `keras3`) :    

```{r echo=TRUE}
tensorflow::tf$config$list_physical_devices("GPU")
```

# Entraîner un premier réseau convolutionnel

Pour notre petite démonstration, nous allons utiliser la célébrissime base de données `mnist` qui contient des images de chiffres écrits à la main. Le but est donc de développer un modèle capable de reconnaître les chiffres manuscrits.  

## Pré-traiter vos données

Les données sont fournies avec 60 000 observations pour les données d'entraînements et 10 000 pour les données de test. Les images étant renseignées directement sous forme de matrice dans le package `keras3` nous n'avons besoin que de normaliser ces matrices sur [0, 1]. On passe aussi la variable d'intérêt, qui donne la valeur du chiffre en question, sous un one-hot-encoder avec la fonction `to_categorical`. On obtient ainsi un vecteur de taille 10 avec 0 ou 1 pour chacune des positions. 

```{r}
library(keras3)

mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test<- mnist$test$x
y_test <- mnist$test$y

# on normalise sur [0, 1]
x_train <- x_train/255
x_test <- x_test/255

# one-hot-encoder pour la variable d'intérêt avec to_categorical
y_train <- to_categorical(y_train, num_classes = 10)
y_test <- to_categorical(y_test, num_classes = 10)

```

## Initialiser et définir votre réseau de neurones profond  

On initialise le modèle avec `keras_model_sequential`, dans lequel on précise d'emblée la dimension des données que le modèle prendra en entrée. Puis on ajoute les différentes couches de neurones.  

Comme nous sommes dans un réseau convolutionnel, on commence directement par une couche de convolution, qui permet de faire ressortir les caractéristiques de chaque image. Cela fonctionne grâce à un système de filtre, réalisé en faisant le produit de la matrice de pixels et d'une matrice plus petite, appelée `feature detector`. On définit dans cette étape le nombre de filtres que l'on souhaite faire passer avec le paramètre `filters` et la taille de la matrice `feature detector` avec le paramètre `kernel_size`.   

On applique ensuite aux matrices obtenues une couche de **max pooling** qui permet de réduire les dimensions du problème en ne conservant que les valeurs les plus importantes. Cette étape permet aussi de rendre plus souvent comparables des images ayant des caractéristiques semblables à des endroits différents de l'image. Le paramètre `pool_size` permet de choisir la taille du sous-ensemble sur lequel on ne va garder que la valeur maximale. 

On peut ensuite refaire des nouvelles couches de convolution suivies de pooling, en fonction de la complexté et de la taille de nos matrices de pixel en entrée du modèle. On termine ces étapes par une couche `flatten()` qui permet d'obtenir un input en deux dimensions sur lequel on va pouvoir construire notre réseau de neurones profond avec des couches `dense()`. On choisit ici une couche avec 128 neurones, suivie d'une couche avec 64 neurones. la couche de sortie contient 10 neurones correspondant aux 10 classes possibles.

Le package R `keras3` permet une syntaxe très proche de celle que l'on utiliserait sur Python. On peut en plus relier ces couches avec l'opérateur `|>`, rendant le code encore plus lisible et aéré : 

```{r}
# Initialisation du modèle  
my_first_r_cnn <- keras_model_sequential(input_shape = c(28,28,1))

# convolution et max pooling
my_first_r_cnn <- my_first_r_cnn |> 
  # on définit les dimension des inputs dans une couche dédiée
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                # l'activation relu "casse" une linéarité qui a pu
                # être introduite avec la convolution
                activation = 'relu') |> 
  layer_max_pooling_2d(pool_size = c(2, 2)) |> 
  layer_conv_2d(filters = 16, kernel_size = c(2, 2), 
                activation = 'relu') |> 
  layer_max_pooling_2d(pool_size = c(2, 2)) |> 
  layer_flatten() |> 
  layer_dense(units = 128, activation = 'relu') |> 
  layer_dense(units = 64, activation = 'relu') |> 
  layer_dense(units = 10, activation = 'softmax')
```

On peut illustrer la structure de notre modèle avec la fonction `summary()` : 

```{r}
summary(my_first_r_cnn)
```


## Entraîner et évaluer les performances de votre modèle

La première étape est de compiler l'objet créé précédemment, en définissant l'_optimizer_ qui sera utilisé, la fonction de perte et la métrique sur laquelle on souhaite optimiser le résultat du modèel :  
  

```{r}
compile(my_first_r_cnn,
        optimizer = 'adam',
        loss = 'categorical_crossentropy',
        metrics = 'accuracy') 
```

Ensuite, on peut lancer l'entraînement avec une instruction `fit()` que l'on applique à nos données d'entraînement. On y définit notamment le paramètre `batch_size` qui permet de déterminer combien d'images le modèle va traiter avant d'ajuster ses coefficients. Le paramètre `epochs` définit le nombre de fois où le modèle passe sur toutes les données. Enfin, on split nos données d'entraînement en conservant 30% comme données de validation afin de d'évaluer correctement les performances de notre modèle :  

```{r}
history <- my_first_r_cnn %>% fit(
  x_train, y_train,
  epochs = 30,
  batch_size = 64,
  validation_split = 0.3)
```

On peut illustrer l'évolution de la qualité du modèle au fur et à mesure des epochs avec `plot` :  

```{r}
plot(history)
```

Enfin, on sort la performance du modèle sur nos données test avec `evaluate` :  

```{r}
accuracy_test <- my_first_r_cnn |> evaluate(x_test, y_test)
print(accuracy_test)
```

> Inutile bien sûr de préciser que nous sommes dans un cas de classification relativement simple et avec énormément de données de bonne qualité, donc ne cherchez pas à atteindre de tels niveaux de précision sur des données un peu plus compliquées! 

# Prédictions à partir du modèle pré-entraîné  

Prenons par exemple la 20e image de l'échantillon test : 

```{r}
index <- 20 # On prend la 20e image"
image_data <- x_test[index,,]  # Sélection de l'image
label <- mnist$test$y[index]  # Label associé

# Inverser les couleurs pour un affichage correct
image_data <- t(apply(image_data, 2, rev))

# Afficher l'image en niveaux de gris
image(image_data, col = gray((0:255)/255), main = paste("Label :", label),
      axes = FALSE, asp = 1)
```

On peut sortir les prédictions des probabilités associées à chaque classe avec la fonction `predict()`. La prédiction associée à l'image précédente donne le résultat suivant :  

```{r}
# On prédit l'ensemble des X_test
matrice_pred <- my_first_r_cnn |> predict(x_test)
# On sort l'index max - 1 (qui correspond à la classe) de l'observation n°20
pred_obs <- which.max(matrice_pred[20,]) - 1

image(image_data, col = gray((0:255)/255), main = paste("Label :", label, 
                                                        "Prédiction :", pred_obs),
      axes = FALSE, asp = 1)

```


# Deep Learning avec R ou Python?  

Le package `keras3` est très complet et semble offrir autant de possibilités que le module Python. Il dispose aussi d'une [documentation complète et très détaillée](https://keras3.posit.co). Cependant, il faut garder en tête qu'il fait tourner Python en arrière-plan.   

Le gros avantage pour un utilisateur non aguerri de Python est que le package `keras3` va gérer pour vous l'environnement anaconda et les dépendances, qui peuvent être un vrai casse-tête quand on se met à Python. Enfin, la syntaxe très proche de celle de Python peut vous permettre de commencer à vous accoutumer en vue d'une future utilisation de Python, tout en restant sur votre langage favori en attendant!  

Enfin, on peut imaginer que l'utilisation d'un package R peut être très pratique dans le cadre de la mise en oeuvre de méthodes de statistiques inférentielles sur des prédictions de modèles de Machine Learning, comme par exemple dans [ce papier très intéressant](https://www.science.org/doi/10.1126/science.adi6000).


